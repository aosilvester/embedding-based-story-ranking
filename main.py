from sentence_transformers  import SentenceTransformer

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

def find_most_relevant(query, text_list):    
    """
    A method for semantically ranking the stories based on relevance to the query topic

    Arguments:
        query (str): The term used to generate the stories.
        text_list (list[str]): A list of stories generated by the tell_a_story_about() method.

    Returns:
        idx (int): The index for the single most relevant story in sims
        sims (numpy.ndarray): An array containing all rankings for each story in the same order as text_list

    """

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = SentenceTransformer('all-MiniLM-L6-v2', device=device)

    encoded_query = model.encode(query, convert_to_tensor=True, device=device)
    embeddings = model.encode(text_list, convert_to_tensor=True, device=device)

    similarities = model.similarity(encoded_query, embeddings)

    sims = similarities.squeeze()
    idx = torch.argmax(sims).item()

    return idx, sims.cpu().numpy()

def tell_a_story_about(query, num_stories = 1, max_length=128):
        """
        A method used for generated stories from the pipeline method of the transformers library

        Arguments:
            query (str): The term used to generate the stories.
            num_stories (int, optional): the number of stories to be generated. defaults to 1 story per method call
            max_length (int, optional): the total number of characters to use for the generated story. defaults to 128 characters

        Returns:
            stories (list[str]): a list of generated stories as strings
            
        """
        prompt = f"Write a short, three-sentence story about {query}. Do make sure to stay on topic and don't provide any meta comments or instructions. Your story is:"
        response = story_teller(prompt, max_new_tokens=max_length, truncation=True, num_return_sequences=num_stories)
        stories = []
        for i in range(num_stories):
            story = response[i]["generated_text"]

            #remove the prompt from the beginning
            story = story[len(prompt):].strip()

            stories.append(story)
        return stories

if __name__ == "__main__":
    print('initializing....')

    tokenizer_name = "microsoft/Phi-4-mini-reasoning"
    model_name = "microsoft/Phi-4-mini-reasoning"

    device = "cuda" if torch.cuda.is_available() else "cpu"
        
    tokenizer = AutoTokenizer.from_pretrained(
        tokenizer_name, 
        trust_remote_code=True
        )
    
    model = AutoModelForCausalLM.from_pretrained(
        "microsoft/Phi-4-mini-reasoning",
        dtype=torch.float16,
        trust_remote_code=True
        ).to("cuda")
    
    story_teller = pipeline(
        "text-generation", 
        model=model, 
        tokenizer=tokenizer,
        device = 0,  # Device 0 for GPU
        do_sample = True, # Enables sampling instead of greedy or beam-based decoding. Crucial for generating diverse outputs.
        top_k = 3, # Limits sampling to the top k most probable next tokens
        temperature = 1.0, # Increased output randomness to have multiple different stories
        )
    
    queries = [
        'mountains',
        'ocean',
        'carrots',
    ]

    for query in queries:
        num_stories = 5

        text_list = tell_a_story_about(query, num_stories=num_stories, max_length=128)
        
        for i in range(num_stories):
            print(f"Story {i}:\n{text_list[i]}\n")
        
        idx, scores = find_most_relevant(query, text_list)

        print(f"idx {idx}, scores {scores}")
